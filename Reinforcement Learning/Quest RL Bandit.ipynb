{"cells":[{"cell_type":"markdown","metadata":{"id":"excxXVzFXgDf"},"source":["# Preparation\n","\n","This notebook implements an epsilon-greedy multi-armed bandit.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VBwBIZPkZf_g"},"source":["Install and import gym library:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MoRJN4PpXsGH"},"outputs":[],"source":["!pip install gym > /dev/null 2>&1\n","import gym"]},{"cell_type":"markdown","metadata":{"id":"WLePIoe7VJXS"},"source":["Gym library provides a large collection of 'environments' with a shared interface to test various reinforcement models. You can find a listing of these environments below, as follows. An environment for bandit problems isn't provided by default and needs to be downloaded and installed:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sGDaa_u8fjO3"},"outputs":[],"source":["!git clone https://github.com/JKCooper2/gym-bandits.git > /dev/null 2>&1\n","!pip install /content/gym-bandits/. > /dev/null 2>&1\n","import gym_bandits"]},{"cell_type":"markdown","metadata":{"id":"nz924RhYdlif"},"source":["This challenge compares 10 bandits and picks the one with the highest payout. Although the distribution is the same for each, the average payout differs."]},{"cell_type":"markdown","metadata":{"id":"0NJTjv1xmi29"},"source":["![alt text](https://i.stack.imgur.com/SazYv.png)"]},{"cell_type":"markdown","metadata":{"id":"Th8WOGSRwAZk"},"source":["Re-run this code to re-initialize the environment at random value:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5nVuGvv0v7gH"},"outputs":[],"source":["import numpy as np\n","np.random.seed(42) "]},{"cell_type":"markdown","metadata":{"id":"6GA0H5L3UNDm"},"source":["Create an variable to contain a 10-arm bandit environment:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJIh1OV_UMMT"},"outputs":[],"source":["env = gym.make('BanditTenArmedGaussian-v0')"]},{"cell_type":"markdown","metadata":{"id":"me9kN1MPWEEf"},"source":["Below is a summary of OpenAI Gym implementation of reinforcement learning, which is going to be useful for the upcoming tasks:"]},{"cell_type":"markdown","metadata":{"id":"PZnKxZw_jWlo"},"source":["1.   **Agent** is a machine learning algorithm\n","2.   An agent runs one of the possible **actions** from the defined list of actions\n","3.   An action is fed to the **environment**, which in our example above is the Atari game environment\n","4.   The environment evaluates the action and produces a **reward** signal (for example, positive or negative.\n","5.   The environment then produces an **observation** representing the current status.\n","6.   The observation & reward signals are fed back to the agent and influence the decition for its next action.\n"]},{"cell_type":"markdown","metadata":{"id":"b-VohcmEjx2p"},"source":["# Tasks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LAIuudWtc-Sx"},"outputs":[],"source":["import numpy as np\n","\n","env.seed(34)\n","\n","numberofbandits = 10\n","q_table = np.zeros(numberofbandits)\n","n_table = np.ones(numberofbandits)\n","\n","epsilon = 0.9"]},{"cell_type":"markdown","metadata":{"id":"AvwzwW3XzwdY"},"source":["Implement the multi-armed bandit model, based on the following pseudo-code: "]},{"cell_type":"markdown","metadata":{"id":"BaKRJx3Ft8US"},"source":["```\n","Create a for loop and run it 1000 times\n","\n","      If statement: generate a random number between 0 and 1; if this number is less than \n","      epsilon, use the best-scoring bandit discovered so far\n","      \n","            Inside the if statement: get the position (index) in our array of the current max value within our\n","            table, this index is the bandit that is giving the best payout so far.\n","            \n","            Inside the if statement: set your action variable equal to the index we discovered in the last \n","            statement\n","            \n","      Else: if the number is greater than or equal to epsilon,\n","      choose a random bandit \n","      \n","          \n","            Inside the if statement: generate a random number between 0 and the total number of bandits\n","            \n","            Inside the if statement: set your action variable to equal the  generated random number.\n","            \n","        \n","            \n","      Inside the loop: feed the action variable into our environment by updating it with a step generated \n","      by either of the steps above\n","      \n","      Inside the loop: now that we have gained some new information from our environment, we need to update our \n","      Q_table. We do this based on the formula: Q_n+1 = Q_n + (R - Q_n), in other words:\n","      NewQvalue = OldQvalue + ((reward - OldQvalue)/numberOfTimesLeverHasBeenPulledForThisBandit)\n","      \n","\n","      Inside the loop: now that we have updated our Q table, we also need to update the table that is keeping\n","      track of how many times each bandit's lever has been pulled. Do this by adding +1 in the position\n","      of our currently selected bandit in the n_table array\n","      \n","      \n","Outside the loop: once everything is done, we would like to print the Bandit with the highest score! Using\n","a print statement, and numpy's argmax function, using our Q table, print the bandit with the highest\n","average payout\n","      \n","   \n","   \n","  \n","```\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7447,"status":"ok","timestamp":1618915037580,"user":{"displayName":"T R","photoUrl":"","userId":"06130323539713709387"},"user_tz":-120},"id":"HonpylcdmhXu","outputId":"f51ab9bc-b01b-4b03-ab7b-0bd7751e03b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Bandint with highes payout: 3\n"]}],"source":["#Create a for loop and run it 1000 times\n","loop_size = 1000\n","for _ in range(loop_size):\n","\n"," # If statement: generate a random number between 0 and 1; if this number is less than \n"," # epsilon, use the best-scoring bandit discovered so far\n"," if random.uniform(0, 1) < epsilon:\n","    \n","    #Inside the if statement: get the position (index) in our array of the current max value within our\n","    #table, this index is the bandit that is giving the best payout so far.\n","    index_current_max = np.argmax(q_table)\n","    \n","    #Inside the if statement: set your action variable equal to the index we discovered in the last \n","    #statement\n","    action = index_current_max\n"," \n"," #Else: if the number is greater than or equal to epsilon, choose a random bandit \n"," #Inside the if statement: generate a random number between 0 and the total number of bandits\n"," #Inside the if statement: set your action variable to equal the  generated random number.\n"," else:  action = random.randrange(len(n_table))\n","     \n"," #Inside the loop: feed the action variable into our environment by updating it with a step generated \n"," #by either of the steps above\n"," new_state, reward, done, info = env.step(action)\n"," \n"," #Inside the loop: now that we have gained some new information from our environment, we need to update our \n"," #Q_table. We do this based on the formula: Q_n+1 = Q_n + (R - Q_n), in other words:\n"," #NewQvalue = OldQvalue + ((reward - OldQvalue)/numberOfTimesLeverHasBeenPulledForThisBandit)\n"," old_qvalue = q_table[action]\n"," new_qvalue = old_qvalue + ((reward - old_qvalue)/ n_table[action])\n"," q_table[action] = new_qvalue\n"," #Inside the loop: now that we have updated our Q table, we also need to update the table that is keeping\n"," #track of how many times each bandit's lever has been pulled. Do this by adding +1 in the position\n"," #of our currently selected bandit in the n_table array\n"," n_table[action] = n_table[action] + 1\n"," \n","#Outside the loop: once everything is done, we would like to print the Bandit with the highest score! Using\n","#a print statement, and numpy's argmax function, using our Q table, print the bandit with the highest\n","#average payout\n","print('Bandint with highes payout:', np.argmax(q_table))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7439,"status":"ok","timestamp":1618915037581,"user":{"displayName":"T R","photoUrl":"","userId":"06130323539713709387"},"user_tz":-120},"id":"DsDvQZ5ipCp2","outputId":"f462b3d9-44fd-4231-bf53-8f50912907f4"},"outputs":[{"data":{"text/plain":["array([ 1.04311304, -0.34239422,  0.09329693,  1.56039608, -0.65421688,\n","       -0.57121621,  1.39800432,  1.46131413, -0.89527164,  0.33579774])"]},"execution_count":83,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["q_table\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7429,"status":"ok","timestamp":1618915037581,"user":{"displayName":"T R","photoUrl":"","userId":"06130323539713709387"},"user_tz":-120},"id":"CalAX10eqnBP","outputId":"990d7c21-3803-48fb-895c-721e07f1622b"},"outputs":[{"data":{"text/plain":["array([ 10.,  11.,  20., 883.,   9.,  10.,  30.,  11.,  10.,  16.])"]},"execution_count":84,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["n_table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bZK5EQ_OtvrU"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Tim_Kopie von Quest RL Bandit.ipynb","provenance":[{"file_id":"196rWEad4k8pqdJgRJFfdjKt_YSgMB6vL","timestamp":1618843220320},{"file_id":"1AgvnqbumrkPAFKI-Apt1SUtvbws4jVSS","timestamp":1603711870492},{"file_id":"1Ety-lOtIJLxz9jkWtw7plMc8EOB2V3wG","timestamp":1549879649414}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.6"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":0}
